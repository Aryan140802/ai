import os
import re
import logging
import subprocess
import traceback
from typing import Dict, Any, Tuple
from datetime import datetime
import json

from langchain_ollama import OllamaLLM

# --- CONFIGURATION ---

# Static token collection for frontend integration
STATIC_TOKENS = {
    "SYSTEM_QUERY": {
        "category": "system",
        "description": "Live system monitoring commands",
        "examples": ["check CPU usage", "memory status", "disk space"]
    },
    "GENERAL_QUERY": {
        "category": "general",
        "description": "General AI assistance",
        "examples": ["explanations", "help", "technical questions"]
    }
}

SAFE_COMMANDS = {
    "cpu": "top -bn1 | grep 'Cpu(s)'",
    "cpu_util": "mpstat 1 1 | grep 'Average' || grep 'all' /proc/stat",
    "memory": "free -m",
    "disk": "df -h",
    "uptime": "uptime",
    "load": "cat /proc/loadavg",
    "processes": "ps aux --sort=-%cpu | head -20",
    "netstat": "ss -tuln | head -20",
    "iostat": "iostat -x 1 1",
    "vmstat": "vmstat 1 2",
    "who": "who",
    "whoami": "whoami",
    "date": "date",
    "hostname": "hostname",
    "uname": "uname -a",
    "lscpu": "lscpu",
    "lsblk": "lsblk",
    "mount": "mount | grep -E '^/dev'",
    "systemctl": "systemctl list-units --type=service --state=active | head -20",
    "pidof": "pidof {process_name}",
    "pgrep": "pgrep -fl {process_name}",
    "ps_pid": "ps -C {process_name} -o pid,cmd --no-headers",
    "topcpu": "ps -eo pid,comm,%cpu,%mem --sort=-%cpu | head -n 11",
    "topmem": "ps -eo pid,comm,%mem,%cpu --sort=-%mem | head -n 11",
    "psaux_grep": "ps aux | grep {process_name} | grep -v grep",
    "top": "top -b -n1 | head -20",
}

BLOCKED_PATTERNS = [
    r"\brm\b", r"\bkill\b", r"\breboot\b", r"\bshutdown\b", r"\buserdel\b",
    r"\bpasswd\b", r"\bmkfs\b", r"\bwget\b", r"\bcurl\b", r":\s*(){:|:&};:",
    r"\bsudo\b", r"\bsu\b", r"\bchmod\b", r"\bchown\b", r"\bdd\b"
]

logging.basicConfig(
    filename=os.path.expanduser("~/.system_ai.log"),
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def is_dangerous(text: str) -> bool:
    return any(re.search(pattern, text.lower()) for pattern in BLOCKED_PATTERNS)

def extract_token_and_query(user_input: str) -> Tuple[str, str]:
    """
    Extract static token and actual query from user input
    Expected format: "TOKEN: actual user query"
    """
    if ":" in user_input:
        parts = user_input.split(":", 1)
        if len(parts) == 2:
            token = parts[0].strip().upper()
            query = parts[1].strip()
            return token, query
    return "", user_input

def get_category_from_token(token: str) -> str:
    """
    Map static token to category
    """
    token_mapping = {
        "SYSTEM_QUERY": "system",
        "GENERAL_QUERY": "general"
    }
    return token_mapping.get(token, "general")

def detect_query_type_fallback(question: str) -> str:
    """
    Fallback detection when no token is provided (for backward compatibility)
    """
    question = question.lower()
    system_patterns = [
        r'\b(cpu|memory|ram|disk|storage|uptime|load|processes|running|network|port|iostat|vmstat)\b',
        r'\b(show|check|what|how much|current|live|real.?time)\b.*\b(cpu|memory|disk|load|system|server)\b',
        r'\b(top|ps|free|df|netstat|who|hostname|uname)\b',
        r'\bsystem\b.*\b(status|info|usage|performance|health)\b',
        r'\b(server|linux|unix)\b.*\b(status|info|performance)\b',
        r'\bhow\s+(much|many)\b.*\b(cpu|memory|disk|process|running)\b'
    ]
    
    for pattern in system_patterns:
        if re.search(pattern, question):
            return "system"
    
    return "general"

def clear_screen():
    os.system("cls" if os.name == "nt" else "clear")

class SystemAIAssistant:
    def __init__(self):
        self.llm = None
        self.initialized = False
        self.chat_history = []

    def initialize(self):
        try:
            print("üîß Initializing System AI Assistant...")
            self.llm = OllamaLLM(model="mistral:7b-instruct-q4_K_M", temperature=0.1)
            self.initialized = True
            print("‚úÖ System AI Assistant initialized successfully!")
            return True
        except Exception as e:
            print(f"‚ùå Initialization failed: {e}")
            logger.error(f"Initialization failed: {e}", exc_info=True)
            return False

    def get_available_tokens(self) -> Dict[str, Dict]:
        """
        Return available static tokens for frontend integration
        """
        return STATIC_TOKENS

    def save_feedback(self, question, answer, feedback):
        data = {
            "question": question,
            "answer": answer,
            "feedback": feedback,
            "timestamp": datetime.now().isoformat()
        }
        try:
            with open("feedback_log.jsonl", "a") as f:
                f.write(json.dumps(data) + "\n")
        except Exception as e:
            logger.error(f"Failed to save feedback: {e}")

    def find_relevant_feedback(self, question):
        try:
            with open("feedback_log.jsonl", "r") as f:
                lines = f.readlines()
            for line in lines[::-1]:
                entry = json.loads(line)
                if entry["question"].strip().lower() in question.strip().lower():
                    return entry["feedback"]
        except Exception:
            pass
        return None

    def run_system_command(self, question: str) -> str:
        question_lower = question.lower()
        command_map = {
            'cpu': ['cpu usage', 'cpu percent', 'cpu utilization', 'cpu load', 'processor usage', 'processor utilization'],
            'cpu_util': ['cpu stat', 'cpu statistics', 'cpu total', 'average cpu'],
            'memory': ['memory usage', 'ram usage', 'mem usage'],
            'disk': ['disk usage', 'storage usage', 'space usage', 'filesystem'],
            'uptime': ['uptime', 'boot time', 'system running'],
            'load': ['load average', 'system load'],
            'processes': ['process list', 'running processes', 'ps', 'processes'],
            'netstat': ['network', 'open port', 'connection', 'port', 'socket'],
            'iostat': ['io', 'input', 'output'],
            'vmstat': ['virtual', 'vm'],
            'who': ['logged user', 'who is logged in', 'session'],
            'hostname': ['hostname', 'host name', 'server name'],
            'uname': ['kernel version', 'os version', 'uname'],
            'lscpu': ['cpu info', 'processor info'],
            'lsblk': ['block device', 'disk device'],
            'mount': ['mounted device', 'mount point'],
            'systemctl': ['service', 'daemon', 'systemctl'],
            'pidof': ['pid of', 'process id of', 'find pid', 'get pid'],
            'pgrep': ['pid for', 'pgrep', 'process name'],
            'ps_pid': ['process id', 'ps -C'],
            'psaux_grep': ['search process', 'grep process', 'find process'],
            'topcpu': ['top cpu', 'most cpu', 'highest cpu', 'max cpu', 'cpu hog'],
            'topmem': ['top memory', 'most memory', 'highest memory', 'max memory', 'memory hog'],
            'top': ['top'],
        }
        
        process_name = None
        cmd = None
        pid_patterns = [
            (r'(?:pid of|process id of|get pid for|find pid for)\s+([a-zA-Z0-9_\-\.]+)', 'pidof'),
            (r'(?:pid for|pgrep|process name)\s+([a-zA-Z0-9_\-\.]+)', 'pgrep'),
            (r'(?:process id|ps -c)\s+([a-zA-Z0-9_\-\.]+)', 'ps_pid'),
            (r'(?:search process|grep process|find process)\s+([a-zA-Z0-9_\-\.]+)', 'psaux_grep')
        ]
        
        for pattern, key in pid_patterns:
            match = re.search(pattern, question_lower)
            if match:
                process_name = match.group(1)
                cmd = SAFE_COMMANDS[key].format(process_name=process_name)
                break
        
        if not cmd:
            matched_cmd = None
            for cmd_key, keywords in command_map.items():
                if any(keyword in question_lower for keyword in keywords):
                    if cmd_key in ['pidof', 'pgrep', 'ps_pid', 'psaux_grep']:
                        proc_match = re.search(r'(?:pid of|get pid for|pgrep|process id for|process id|ps -c|search process|grep process|find process)\s+([a-zA-Z0-9_\-\.]+)', question_lower)
                        if proc_match:
                            process_name = proc_match.group(1)
                            cmd = SAFE_COMMANDS[cmd_key].format(process_name=process_name)
                            matched_cmd = cmd
                            break
                        else:
                            continue
                    else:
                        matched_cmd = SAFE_COMMANDS.get(cmd_key)
                        break
            if not matched_cmd:
                matched_cmd = SAFE_COMMANDS['processes']
            if not cmd:
                cmd = matched_cmd
        
        try:
            output = subprocess.getoutput(cmd)
            context = f"""
System command executed: {cmd}
Output: {output}

User question: {question}

Please provide a clear, helpful response that directly answers the user's question based on this system information. Be concise but informative.
"""
            feedback = self.find_relevant_feedback(question)
            if feedback:
                context += f"\nNote: Previously, a user provided this correction for a similar question: '{feedback}'"
            
            ai_response = self.llm.invoke(context)
            self.chat_history.append({"user": question, "assistant": ai_response})
            return ai_response
        except Exception as e:
            logger.error(f"System command error: {e}")
            return f"‚ùå Error getting system information: {e}"

    def general_ai_response(self, question: str) -> str:
        try:
            context = f"""
You are a helpful AI assistant with expertise in system administration and general technical knowledge.

User question: {question}

Please provide a clear, helpful, and accurate response. If this is a technical question, provide practical advice. If it's a general question, be informative and conversational.
"""
            feedback = self.find_relevant_feedback(question)
            if feedback:
                context += f"\nNote: Previously, a user provided this correction for a similar question: '{feedback}'"
            
            response = self.llm.invoke(context)
            self.chat_history.append({"user": question, "assistant": response})
            return response
        except Exception as e:
            logger.error(f"AI response error: {e}")
            return f"‚ùå Unable to process your question: {e}"

    def process_question_with_token(self, user_input: str) -> Dict[str, Any]:
        """
        Main method for frontend integration - processes question with static token
        Returns structured response for API consumption
        """
        if not self.initialized:
            return {
                "success": False,
                "error": "Assistant not initialized",
                "response": "‚ùå Assistant not initialized. Please restart."
            }
        
        if is_dangerous(user_input):
            return {
                "success": False,
                "error": "Blocked for security",
                "response": "üö´ Question blocked for security reasons."
            }
        
        # Extract token and query
        token, actual_query = extract_token_and_query(user_input)
        
        # Determine category from token or fallback detection
        if token and token in ["SYSTEM_QUERY", "GENERAL_QUERY"]:
            category = get_category_from_token(token)
        else:
            # Fallback to automatic detection
            category = detect_query_type_fallback(actual_query)
            token = "AUTO_DETECTED"
        
        try:
            # Process based on category
            if category == "system":
                response = self.run_system_command(actual_query)
            else:
                response = self.general_ai_response(actual_query)
            
            return {
                "success": True,
                "token_used": token,
                "category": category,
                "original_query": actual_query,
                "response": response,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Processing error: {e}")
            return {
                "success": False,
                "error": str(e),
                "response": f"‚ùå Error processing your request: {e}"
            }

    def process_question(self, question: str) -> str:
        """
        Backward compatibility method
        """
        result = self.process_question_with_token(question)
        return result["response"]

    def show_tokens(self):
        """
        Display available tokens for reference
        """
        print("üè∑Ô∏è  AVAILABLE STATIC TOKENS:")
        print("=" * 50)
        for token, info in STATIC_TOKENS.items():
            print(f"Token: {token}")
            print(f"Category: {info['category']}")
            print(f"Description: {info['description']}")
            print(f"Examples: {', '.join(info['examples'])}")
            print("-" * 30)

    def show_help(self):
        help_text = """
üìñ SYSTEM AI ASSISTANT HELP (Frontend Ready)

üè∑Ô∏è  STATIC TOKENS (for Frontend Integration):
  Usage: "TOKEN: your question"
  
  Available Tokens:
  - SYSTEM_QUERY: For live system monitoring
  - GENERAL_QUERY: For general AI assistance

üñ•Ô∏è  SYSTEM COMMANDS (use SYSTEM_QUERY token):
  CPU Information:
  - "SYSTEM_QUERY: Show CPU usage"
  - "SYSTEM_QUERY: What's the CPU utilization?"
  - "SYSTEM_QUERY: Check processor info"
  
  Memory Information:
  - "SYSTEM_QUERY: Check memory status"
  - "SYSTEM_QUERY: How much RAM is used?"
  - "SYSTEM_QUERY: Show memory usage"
  
  Disk Information:
  - "SYSTEM_QUERY: Check disk space"
  - "SYSTEM_QUERY: Show filesystem usage"
  - "SYSTEM_QUERY: What's the storage status?"
  
  Process Information:
  - "SYSTEM_QUERY: What processes are running?"
  - "SYSTEM_QUERY: Show top CPU processes"
  - "SYSTEM_QUERY: List memory hogs"
  - "SYSTEM_QUERY: Find PID of apache"
  
  System Information:
  - "SYSTEM_QUERY: Show system uptime"
  - "SYSTEM_QUERY: What's the load average?"
  - "SYSTEM_QUERY: Check hostname"
  - "SYSTEM_QUERY: Show kernel version"
  
  Network Information:
  - "SYSTEM_QUERY: Show open ports"
  - "SYSTEM_QUERY: Check network connections"
  - "SYSTEM_QUERY: List active sockets"

ü§ñ GENERAL AI (use GENERAL_QUERY token):
  - "GENERAL_QUERY: Explain Linux commands"
  - "GENERAL_QUERY: How do I monitor system performance?"
  - "GENERAL_QUERY: What is load average?"

üí° COMMANDS:
  - 'help' - Show this help
  - 'tokens' - Show available tokens
  - 'clear' - Clear screen
  - 'status' - Show system status
  - 'exit' - Quit assistant

üì° API Integration:
  Use process_question_with_token() method for structured responses

üîí SECURITY:
  Only safe, read-only system commands are allowed.
  Destructive commands (rm, kill, sudo, etc.) are blocked.
        """
        print(help_text)

    def show_status(self):
        print("üîç SYSTEM STATUS")
        print(f"üìÖ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ü§ñ AI Model: Initialized ({'‚úÖ' if self.initialized else '‚ùå'})")
        print(f"üè∑Ô∏è  Available Tokens: {len(STATIC_TOKENS)}")
        print(f"üõ°Ô∏è  Safe Commands Available: {len(SAFE_COMMANDS)}")
        print(f"üö´ Blocked Patterns: {len(BLOCKED_PATTERNS)}")

    def start_interactive_session(self):
        if not self.initialize():
            return
        clear_screen()
        print("ü§ñ System AI Assistant Ready (Frontend Integration Enabled)")
        print("Ask me about system status, performance, or general questions...")
        print("Type 'exit' to quit, 'tokens' to see available tokens\n")
        
        while True:
            try:
                question = input("üí¨ ").strip()
                if not question:
                    continue
                
                question_lower = question.lower()
                if question_lower in ['exit', 'quit', 'q']:
                    print("üëã Goodbye!")
                    break
                elif question_lower == 'help':
                    self.show_help()
                    continue
                elif question_lower == 'tokens':
                    self.show_tokens()
                    continue
                elif question_lower == 'clear':
                    clear_screen()
                    continue
                elif question_lower == 'status':
                    self.show_status()
                    continue
                
                result = self.process_question_with_token(question)
                print(f"\nüè∑Ô∏è  Token Used: {result.get('token_used', 'N/A')}")
                print(f"üìÇ Category: {result.get('category', 'N/A')}")
                print(f"üìù Response: {result['response']}\n")
                
                if result['success']:
                    feedback = input("Was this answer helpful? (yes/no/correction): ")
                    if feedback.lower() not in ['yes', 'y']:
                        self.save_feedback(result.get('original_query', question), result['response'], feedback)
                        
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                logger.error(f"Session error: {e}", exc_info=True)

def main():
    assistant = SystemAIAssistant()
    
    # Example usage for frontend integration
    print("=== Frontend Integration Examples ===")
    assistant.initialize()
    
    # Example 1: With static token
    example1 = "SYSTEM_QUERY: What's the CPU usage?"
    result1 = assistant.process_question_with_token(example1)
    print(f"Input: {example1}")
    print(f"Result: {result1}")
    print()
    
    # Example 2: Without token (auto-detection)
    example2 = "Check memory usage"
    result2 = assistant.process_question_with_token(example2)
    print(f"Input: {example2}")
    print(f"Result: {result2}")
    print()
    
    # Start interactive session
    assistant.start_interactive_session()

if __name__ == "__main__":
    main()
